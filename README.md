<h1>Boosting and Bagging Method</h1>

In Machine Learning, a person cannot make the predictions with a model lonely. So, to prevent from biases and variances, <i>Ensemble Learning</i> was introduced. Ensemble Learning defines that combining a number of models to make the predictions to be more flexibility and less variance. 
This learning has two methods:
1. Bagging  : Training a set of models in <b>Parallel way</b>. Each model is trained by a random subset of data.
2. Boosting : Training a set of models in <b>Sequential way</b>. Each model learns from mistakes of the previous models.

<h2>Random Forest</h2>
An Ensemble model which uses bagging method and holds Decision Trees as individual models.

<h2>Adaptive Boosting</h2>
As I said early, boosting learns from its mistakes in previous models, Adaboost in one of that variety. The errors are misclassified data points, which has to be weighted through the process.

<h2>Gradient Boosting</h2>
Is also using boosting method, which learns from previous mistake directly with the residua errors instead of weighing every data points.


